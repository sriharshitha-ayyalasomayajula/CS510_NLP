{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc12277f",
   "metadata": {},
   "source": [
    "         Sarcasam Classification using Naive Bayes and Logistic Regression models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af1d25",
   "metadata": {},
   "source": [
    "Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd5f47",
   "metadata": {},
   "source": [
    "Goal:\n",
    "The goal of this assignment is to build a Naive Bayes and Logistic regression classification models that will predict if the piece of text is sarcastic or not. We use different features as input to the models and expect the output to be sarcastic or not. We evaluate the model using 10 fold cross validation and the metrics are accuracy and F-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90c1f7",
   "metadata": {},
   "source": [
    "Dataset: The dataset used for this assignmnet is news headlines which is collected from two news website: The Onion2, which aims at producing sarcastic versions of current events, and HuffPost3, which provides the set of real (and non-sarcastic) news headlines.\n",
    "Source: https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afee898",
   "metadata": {},
   "source": [
    "Programming Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5bc947",
   "metadata": {},
   "source": [
    "Step 1: Import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a60232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5d7dba",
   "metadata": {},
   "source": [
    "Step 2: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08f6078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  thirtysomething scientists unveil doomsday clo...   \n",
       "1             0  dem rep. totally nails why congress is falling...   \n",
       "2             0  eat your veggies: 9 deliciously different recipes   \n",
       "3             1  inclement weather prevents liar from getting t...   \n",
       "4             1  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "                                        article_link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(r'/Users/sriharshithaayyalasomayajula/Desktop/Sarcasm_Headlines_Dataset.json', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea64256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_sarcastic    False\n",
      "headline        False\n",
      "article_link    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "#Checking if there are any null values in the data\n",
    "print(df.isnull().any(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e391f9",
   "metadata": {},
   "source": [
    "Step 3: Lets define the corpus here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc5a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the corpus \n",
    "corpus = []\n",
    "for i in range(0, len(df)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', df['headline'][i])\n",
    "    review = review.lower()\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea506216",
   "metadata": {},
   "source": [
    "Step 4: Feature Engineering with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6deed62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "Name: is_sarcastic, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Ngrams as features, fitting the data using countvectorizer\n",
    "#In here I'm using all the three uni,bi and tri \n",
    "\n",
    "cv = CountVectorizer(max_features=5000,ngram_range=(1,3))\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = df['is_sarcastic']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52d39aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting the data into test and train (80,20)\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,random_state = 0,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a86797c",
   "metadata": {},
   "source": [
    "Step 5: Classification models and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0c79f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes Model: 0.826869322152341\n",
      "F Score of Naive Bayes Model: 0.8175290001841282\n",
      "Accuracy of Naive Bayes Model with 10 fold cross Validation: 0.8368914735896198\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier with N-grams \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "modelNB = MultinomialNB().fit(x_train,y_train)\n",
    "y_pred = modelNB.predict(x_test)\n",
    "\n",
    "#Accuracy and F1 Score for Naive Bayes\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "score = metrics.accuracy_score(y_test,y_pred)\n",
    "f1_score = metrics.f1_score(y_test,y_pred)\n",
    "print(\"Accuracy of Naive Bayes Model:\", score)\n",
    "print(\"F Score of Naive Bayes Model:\", f1_score)\n",
    "\n",
    "#Naive Baye's with K-fold Cross validation \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model1 = MultinomialNB().fit(X,y)\n",
    "score_NB = cross_val_score(model1,X,y,cv = 10)\n",
    "print(\"Accuracy of Naive Bayes Model with 10 fold cross Validation:\", score_NB.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2b1e711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression model: 0.826869322152341\n",
      "F Score of Logistic Regression model: 0.8175290001841282\n",
      "Accuracy of LR Model with 10 fold cross validation: 0.8388481960952993\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model with N-grams \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "modelLR = LogisticRegression(solver='liblinear', random_state=0).fit(x_train, y_train)\n",
    "y_predLR = modelLR.predict(x_test)\n",
    "\n",
    "#Accuracy and F1 Score for Logistic Regression\n",
    "\n",
    "score_LR = metrics.accuracy_score(y_test,y_predLR)\n",
    "f1_score_LR = metrics.f1_score(y_test,y_predLR)\n",
    "print(\"Accuracy of Logistic Regression model:\", score)\n",
    "print(\"F Score of Logistic Regression model:\", f1_score)\n",
    "\n",
    "#Logistic Regression with K-fold Cross validation\n",
    "\n",
    "model2 = LogisticRegression(solver='liblinear', random_state=0).fit(X,y)\n",
    "score_LRM = cross_val_score(model2,X,y,cv=10)\n",
    "print(\"Accuracy of LR Model with 10 fold cross validation:\", score_LRM.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1ca9322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.836891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.838848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Models  Accuracy\n",
       "0          Naive Bayes  0.836891\n",
       "1  Logistic Regression  0.838848"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = ['Naive Bayes', 'Logistic Regression']\n",
    "col = [score_NB.mean() ,score_LRM.mean() ]\n",
    "data ={'Models': models, 'Accuracy': col}\n",
    "graph_df = pd.DataFrame(data)\n",
    "graph_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef4955",
   "metadata": {},
   "source": [
    "Written Component:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99931a19",
   "metadata": {},
   "source": [
    "1. In here I have used a combination of uni,bi and trigrams as features. The accuracy after evaluation is 83.6% for NB classifier and 83.88% for LR classifer. I have experimented with other combinations of n-grams as well - for example with uni and bi accuracy for both classifiers was in betwween 69% and 73%, with bi and tri grams it was between 79% and 82%. So, I have chosen to go with a combination of all the three. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad0524",
   "metadata": {},
   "source": [
    "2. For this data, the best model is Logistic Regression. LR model yields the best results for boolean classification examples. Even though both LR and NB are used for linear classification, LR uses a direct function of the probability of classifiying correctly to do the predictions. In here I have used N-grams as features and have achieve accuracy of 83.88%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf6753",
   "metadata": {},
   "source": [
    "3. For error analysis, we find that doing various effective pre-processing methods would help to reduce errors. For instance if we have weblinks (which are present in the dataset that we used), the model also will consider the https etc, which would reduce the performance(if not needed then drop the coulmn, in this case article link column can be dropped). By removing stop words,PoS(in this case), links, any emoticons, punctuations etc. can help to correctly predict the piece of text to be sarcastic or not as well as improve the accuracy of the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9152b",
   "metadata": {},
   "source": [
    "4. For future work, I'd like to use stop words, punctuations  and also see how embeddings would effect the performance of the models. I would also like to see how a neural network model would work with these features and if there would be any increase in accuracy compared to the linear classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e36d8",
   "metadata": {},
   "source": [
    "References: \n",
    "1. https://scikit-learn.org/stable/modules/cross_validation.html?highlight=f1+score\n",
    "2. https://thinkingneuron.com/how-to-generate-n-grams-in-python/\n",
    "3. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
